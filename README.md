# Object Detection Based on Caption with YOLOv3

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

## Overview

Welcome to the "Object Detection Based on Caption with YOLOv3" project. This project demonstrates an innovative approach to object detection by utilizing the power of YOLOv3 (You Only Look Once) and natural language processing techniques. By extracting information from captions associated with images, we aim to accurately detect and label objects present in the images.

## Project Description

In this project, we explore the fusion of object detection and language processing to achieve the following:

- Detect and locate objects in images based on their associated captions.
- Utilize the pre-trained YOLOv3 model for object detection tasks.
- Develop a custom model architecture using inspiration from [keras-yolo3](https://github.com/experiencor/keras-yolo3).
- Process captions to identify objects mentioned in them.
- Create a dataset combining image information and object labels from captions.
- Perform one-hot encoding for each label to reflect object presence in captions.

## Model Creation and Usage

To achieve object detection based on captions, we follow these steps:

1. Create a custom YOLOv3 model using the architecture inspired by [keras-yolo3](https://github.com/experiencor/keras-yolo3). The model weights can be downloaded from the YOLO website.
2. Process and tokenize captions, and map objects to their synonyms using NLTK library and manual additions.
3. Create a dictionary to associate YOLO object labels with their alternative words.
4. Perform tokenization and lemmatization of captions to normalize words.
5. Build a sample dataset by encoding objects mentioned in captions as one-hot vectors.
6. Utilize the saved model to detect objects in images based on their captions.

## Installation and Setup

To replicate the project and experiment with the model, follow these steps:

1. Clone this repository: `git clone https://github.com/Delavari-Alireza/OBJECT_DETECTION_BASED_ON_CAPTION.git`
2. Install the required dependencies: `pip install -r requirements.txt`
3. Download the pre-trained YOLOv3 weights and save them in the designated directory.
4. Review and adapt the Jupyter Notebook files to your dataset and requirements.
5. Open the Jupyter Notebook: `jupyter notebook`
6. Run and experiment with the notebook files.

## Sample Results

This project aims to showcase how the synergy between object detection models and natural language processing can yield accurate results in identifying objects based on their associated captions. Examples of object detection based on captions are available in the Jupyter Notebook.

## License

This project is licensed under the MIT license. For more information, please refer to the license file.

## Acknowledgments

We acknowledge the creators of YOLOv3 and the [keras-yolo3](https://github.com/experiencor/keras-yolo3) repository for their valuable contributions to the field of computer vision and object detection.

For detailed implementation insights and methodology specifics, please refer to the Jupyter Notebook files and available documentation in this repository.

---

Feel free to explore the notebooks, contribute to the project, or adapt the techniques for your own object detection and language processing scenarios!
